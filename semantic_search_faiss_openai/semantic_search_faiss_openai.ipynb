{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013690e6",
   "metadata": {},
   "source": [
    "\n",
    "# Semantic Search Engine (OpenAI Embeddings + FAISS)\n",
    "\n",
    "This notebook implements a **semantic search engine** that encodes documents and queries with **OpenAI embeddings** and performs fast nearest-neighbor retrieval with **FAISS**.\n",
    "\n",
    "**What you'll get:**\n",
    "- Dataset loader for 200+ documents (a ready-made sample corpus is included, or plug in your own).\n",
    "- Batched embedding of documents with OpenAI (robust to timeouts; averages chunk embeddings per doc).\n",
    "- FAISS index creation (cosine-similarity equivalent via inner product on L2-normalized vectors).\n",
    "- A clean search interface that returns the most semantically relevant documents.\n",
    "- Five diverse sample queries with ranked outputs.\n",
    "- Clear comments and explanations of the full flow from input → embeddings → index → search results.\n",
    "\n",
    "> **Tip:** If you do not yet have an OpenAI API key or FAISS installed, the notebook can fall back to a pure-NumPy demo embedding/index so you can still run the end-to-end flow. For your submission, run the cells in the **OpenAI + FAISS** configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf415b1",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Python packages** (install in a terminal or first notebook cell):\n",
    "```bash\n",
    "pip install -U openai faiss-cpu numpy pandas tqdm pyarrow tiktoken\n",
    "# If you use conda/mamba, on some platforms:\n",
    "# mamba install -c conda-forge -c pytorch faiss-cpu\n",
    "```\n",
    "\n",
    "**OpenAI API key**\n",
    "- Set an environment variable before launching the notebook:\n",
    "  - macOS/Linux: `export OPENAI_API_KEY=\"sk-...\"`\n",
    "  - Windows (PowerShell): `$Env:OPENAI_API_KEY=\"sk-...\"`\n",
    "- Or, you can paste it into the notebook when prompted (not recommended for shared environments).\n",
    "\n",
    "> **Costs:** Embedding 200–500 short documents with `text-embedding-3-small` is typically inexpensive. Always review current pricing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a618b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OPTIONAL: Install dependencies in this environment (uncomment if needed).\n",
    "# In some hosted notebooks (like Colab), you may need this.\n",
    "# %pip -q install -U openai faiss-cpu numpy pandas tqdm pyarrow tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc59c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, re, json, math, time, textwrap, glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Try OpenAI SDK (new style, >= 1.0)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    _openai_available = True\n",
    "except Exception:\n",
    "    _openai_available = False\n",
    "    OpenAI = None\n",
    "\n",
    "# Try FAISS\n",
    "try:\n",
    "    import faiss  # type: ignore\n",
    "    _faiss_available = True\n",
    "except Exception:\n",
    "    _faiss_available = False\n",
    "    faiss = None  # type: ignore\n",
    "\n",
    "# ---------------- Configuration ----------------\n",
    "DATA_DIR = Path(\"./sample_corpus\")           # Folder of .txt files (each file = one document)\n",
    "EMBED_CACHE = Path(\"./embeddings.npy\")       # Optional cache of document embeddings\n",
    "META_CACHE = Path(\"./doc_meta.parquet\")      # Optional cache of document metadata\n",
    "INDEX_PATH = Path(\"./faiss.index\")           # Optional saved FAISS index\n",
    "\n",
    "# Embedding model (OpenAI). \"text-embedding-3-small\" (1536 dims) is a solid default.\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# Fallback embedding dimension for local demo embedding (hashing trick)\n",
    "FALLBACK_DIM = 1024\n",
    "\n",
    "# Toggles (the notebook will auto-detect what is possible):\n",
    "USE_OPENAI = True        # will flip to False if no key or SDK unavailable\n",
    "USE_FAISS  = True        # will flip to False if FAISS not installed\n",
    "\n",
    "# Detect API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "if not _openai_available or not OPENAI_API_KEY:\n",
    "    USE_OPENAI = False\n",
    "\n",
    "if not _faiss_available:\n",
    "    USE_FAISS = False\n",
    "\n",
    "print(f\"OpenAI available: {_openai_available}, key found: {bool(OPENAI_API_KEY)} -> USE_OPENAI={USE_OPENAI}\")\n",
    "print(f\"FAISS available: {_faiss_available} -> USE_FAISS={USE_FAISS}\")\n",
    "\n",
    "# Make directories relative to notebook location\n",
    "DATA_DIR = Path(DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d3b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l2_normalize(mat: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"Row-wise L2 normalization.\"\"\"\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True) + eps\n",
    "    return mat / norms\n",
    "\n",
    "def batched(iterable, n: int):\n",
    "    \"\"\"Yield successive n-sized batches from an iterable.\"\"\"\n",
    "    batch = []\n",
    "    for item in iterable:\n",
    "        batch.append(item)\n",
    "        if len(batch) == n:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def read_text_file(path: Path) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def preview(text: str, n: int = 240) -> str:\n",
    "    t = re.sub(r\"\\s+\", \" \", text.strip())\n",
    "    return (t[:n] + \"…\") if len(t) > n else t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5753b4",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Data loading & preprocessing\n",
    "\n",
    "This section loads a folder of `.txt` files (each treated as an independent document).  \n",
    "The included **`sample_corpus/`** has 240+ documents across many topics.\n",
    "\n",
    "Feel free to replace `DATA_DIR` with your own directory, or adapt the loader to read from CSV/JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c391ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the sample corpus exists (if you downloaded the zip next to this notebook, unzip it here).\n",
    "# If you cloned this notebook with the provided zip, run the next cell to confirm the file count.\n",
    "DATA_DIR = Path(\"./sample_corpus\")\n",
    "if not DATA_DIR.exists():\n",
    "    # If running this notebook in the same folder as the zip provided,\n",
    "    # you can unzip it like so (uncomment):\n",
    "    # import zipfile\n",
    "    # with zipfile.ZipFile(\"sample_corpus.zip\", \"r\") as zf:\n",
    "    #     zf.extractall(\".\")\n",
    "    pass\n",
    "\n",
    "all_files = sorted([Path(p) for p in glob.glob(str(DATA_DIR / \"**\" / \"*.txt\"), recursive=True)])\n",
    "print(f\"Found {len(all_files)} text files in {DATA_DIR!s}\")\n",
    "assert len(all_files) >= 200, \"Please provide at least 200 documents.\"\n",
    "\n",
    "# Read the documents into a DataFrame with columns: id, path, title, text, n_chars, n_words, category\n",
    "rows = []\n",
    "for doc_id, p in enumerate(all_files):\n",
    "    txt = read_text_file(p)\n",
    "    # title = first line or Title: ... fallback\n",
    "    m = re.search(r\"^Title:\\s*(.*?)$\", txt, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    title = m.group(1).strip() if m else (txt.splitlines()[0].strip() if txt.strip() else p.stem)\n",
    "    m2 = re.search(r\"^Category:\\s*(.*?)$\", txt, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    category = m2.group(1).strip() if m2 else (p.parent.name if p.parent.name else \"unknown\")\n",
    "    n_chars = len(txt)\n",
    "    n_words = len(re.findall(r\"\\w+\", txt))\n",
    "    rows.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"path\": str(p),\n",
    "        \"title\": title,\n",
    "        \"text\": txt,\n",
    "        \"n_chars\": n_chars,\n",
    "        \"n_words\": n_words,\n",
    "        \"category\": category\n",
    "    })\n",
    "\n",
    "docs = pd.DataFrame(rows)\n",
    "docs.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba367ca",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Embedding generation\n",
    "\n",
    "We convert each document into a dense vector so we can search by **meaning** rather than keywords.\n",
    "\n",
    "**OpenAI path (recommended):**\n",
    "- We chunk long documents (by characters) to keep embeddings efficient.\n",
    "- Embed each chunk with `text-embedding-3-small` (or switch to `text-embedding-3-large`).\n",
    "- Aggregate a **single vector per document** by averaging its chunk embeddings.\n",
    "- Cache the resulting matrix and metadata to speed up re-runs.\n",
    "\n",
    "**Fallback (demo mode, no API):**\n",
    "- A lightweight **hashing trick** turns tokens into a fixed-size vector (1024 dims by default).\n",
    "- This is not as accurate as OpenAI embeddings but keeps the end-to-end flow runnable anywhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text_by_chars(text: str, max_chars: int = 2000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks by character count.\"\"\"\n",
    "    text = text.strip()\n",
    "    if len(text) <= max_chars:\n",
    "        return [text]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + max_chars)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        if end == len(text):\n",
    "            break\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return chunks\n",
    "\n",
    "# ---------- OpenAI embedding (if enabled) ----------\n",
    "_openai_client = None\n",
    "if USE_OPENAI:\n",
    "    _openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def embed_openai(texts: List[str], model: str = EMBEDDING_MODEL, batch_size: int = 96, max_retries: int = 5) -> np.ndarray:\n",
    "    \"\"\"Embed a list of strings with OpenAI, returning a float32 matrix of shape [N, D].\"\"\"\n",
    "    if not USE_OPENAI or _openai_client is None:\n",
    "        raise RuntimeError(\"OpenAI embedding requested but client not available.\")\n",
    "    out = []\n",
    "    for batch in tqdm(list(batched(texts, batch_size)), desc=\"Embedding (OpenAI)\"):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                resp = _openai_client.embeddings.create(model=model, input=batch)\n",
    "                # The Embeddings API returns a list in resp.data mirroring input order\n",
    "                vecs = [np.array(d.embedding, dtype=np.float32) for d in resp.data]\n",
    "                out.extend(vecs)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                wait = min(60, 2 ** attempt)\n",
    "                print(f\"OpenAI error: {e}. Retrying in {wait}s (attempt {attempt+1}/{max_retries})...\")\n",
    "                time.sleep(wait)\n",
    "        else:\n",
    "            raise RuntimeError(\"OpenAI embedding failed after retries.\")\n",
    "    return np.vstack(out)\n",
    "\n",
    "def embed_documents_openai(docs_df: pd.DataFrame) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Compute one embedding per document by averaging chunk embeddings.\"\"\"\n",
    "    all_chunks = []\n",
    "    doc_ptrs = []\n",
    "    for _, row in docs_df.iterrows():\n",
    "        chunks = chunk_text_by_chars(row[\"text\"], max_chars=2000, overlap=200)\n",
    "        doc_ptrs.append((len(all_chunks), len(all_chunks) + len(chunks)))\n",
    "        all_chunks.extend(chunks)\n",
    "    chunk_matrix = embed_openai(all_chunks, model=EMBEDDING_MODEL)\n",
    "    # Average chunks per doc\n",
    "    doc_vecs = []\n",
    "    for start, end in doc_ptrs:\n",
    "        vec = chunk_matrix[start:end].mean(axis=0)\n",
    "        doc_vecs.append(vec.astype(np.float32))\n",
    "    mat = np.vstack(doc_vecs)\n",
    "    return mat, mat.shape[1]\n",
    "\n",
    "# ---------- Fallback: hashing-trick embedding (no API needed) ----------\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"[a-zA-Z0-9_]+\", text.lower())\n",
    "\n",
    "def embed_hashing(texts: List[str], dim: int = FALLBACK_DIM, salt: int = 13) -> np.ndarray:\n",
    "    \"\"\"A simple, deterministic feature-hashing embedding for demo mode.\"\"\"\n",
    "    mat = np.zeros((len(texts), dim), dtype=np.float32)\n",
    "    for i, t in enumerate(texts):\n",
    "        tokens = _tokenize(t)\n",
    "        for tok in tokens:\n",
    "            h = int(hashlib.md5((tok + str(salt)).encode(\"utf-8\")).hexdigest(), 16)\n",
    "            idx = h % dim\n",
    "            mat[i, idx] += 1.0\n",
    "        # Log-scaling + L2 normalization improves behavior a bit\n",
    "        mat[i, :] = np.log1p(mat[i, :])\n",
    "    return mat\n",
    "\n",
    "def embed_documents_fallback(docs_df: pd.DataFrame, dim: int = FALLBACK_DIM) -> Tuple[np.ndarray, int]:\n",
    "    texts = docs_df[\"text\"].tolist()\n",
    "    mat = embed_hashing(texts, dim=dim)\n",
    "    return mat, dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f016dbf",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Build (or load) document embeddings\n",
    "\n",
    "This will compute one vector per document and cache it to speed up future runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c489de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if EMBED_CACHE.exists() and META_CACHE.exists():\n",
    "    print(f\"Loading cached embeddings from {EMBED_CACHE} and metadata from {META_CACHE}\")\n",
    "    doc_embeddings = np.load(EMBED_CACHE)\n",
    "    docs = pd.read_parquet(META_CACHE)\n",
    "    vector_dim = doc_embeddings.shape[1]\n",
    "else:\n",
    "    if USE_OPENAI:\n",
    "        print(\"Using OpenAI embeddings...\")\n",
    "        doc_embeddings, vector_dim = embed_documents_openai(docs)\n",
    "    else:\n",
    "        print(\"Using fallback hashing embeddings (demo mode)...\")\n",
    "        doc_embeddings, vector_dim = embed_documents_fallback(docs, dim=FALLBACK_DIM)\n",
    "\n",
    "    # Normalize for cosine similarity via inner product in FAISS\n",
    "    doc_embeddings = l2_normalize(doc_embeddings.astype(np.float32))\n",
    "\n",
    "    # Cache\n",
    "    np.save(EMBED_CACHE, doc_embeddings)\n",
    "    docs.to_parquet(META_CACHE, index=False)\n",
    "\n",
    "doc_embeddings.shape, vector_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc94da6",
   "metadata": {},
   "source": [
    "\n",
    "## 4) FAISS index creation\n",
    "\n",
    "We build an **inner-product (IP)** index over **L2-normalized** vectors. This makes IP scores equal to cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NumpyFlatIndex:\n",
    "    \"\"\"Minimal in-memory index as a fallback when FAISS is unavailable.\"\"\"\n",
    "    def __init__(self, xb: np.ndarray):\n",
    "        self.xb = xb.astype(np.float32)\n",
    "    def search(self, q: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        # q: [B, D], xb: [N, D] -> scores [B, N] via dot product (cosine if vectors are normalized)\n",
    "        scores = q @ self.xb.T\n",
    "        # Top-k\n",
    "        idx = np.argpartition(-scores, kth=np.minimum(k, scores.shape[1]-1), axis=1)[:, :k]\n",
    "        # sort within the top-k for each query\n",
    "        sorted_idx = np.take_along_axis(idx, np.argsort(-np.take_along_axis(scores, idx, axis=1)), axis=1)\n",
    "        sorted_scores = np.take_along_axis(scores, sorted_idx, axis=1)\n",
    "        return sorted_scores, sorted_idx\n",
    "\n",
    "# Build the index\n",
    "if USE_FAISS:\n",
    "    index = faiss.IndexFlatIP(vector_dim)\n",
    "    index.add(doc_embeddings)  # doc_ids correspond to row indices\n",
    "    print(\"FAISS index built:\", type(index), \"| size:\", index.ntotal)\n",
    "    # Optionally persist\n",
    "    try:\n",
    "        faiss.write_index(index, str(INDEX_PATH))\n",
    "        print(f\"Saved FAISS index to {INDEX_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not save FAISS index:\", e)\n",
    "else:\n",
    "    index = NumpyFlatIndex(doc_embeddings)\n",
    "    print(\"Using NumpyFlatIndex fallback (FAISS not available). Size:\", doc_embeddings.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0bf43",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Query interface\n",
    "\n",
    "`search(query, k)`:\n",
    "- Embed the query (OpenAI or fallback)\n",
    "- L2-normalize the query vector\n",
    "- Lookup top-`k` nearest neighbors in FAISS\n",
    "- Return a tidy pandas DataFrame with `rank`, `score`, `title`, `category`, `path`, and a preview snippet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_query(text: str) -> np.ndarray:\n",
    "    if USE_OPENAI:\n",
    "        q = embed_openai([text], model=EMBEDDING_MODEL)\n",
    "    else:\n",
    "        q = embed_hashing([text], dim=doc_embeddings.shape[1])\n",
    "    q = l2_normalize(q.astype(np.float32))\n",
    "    return q\n",
    "\n",
    "def search(query: str, k: int = 5) -> pd.DataFrame:\n",
    "    qv = embed_query(query)\n",
    "    D, I = index.search(qv, k)\n",
    "    # single-query case\n",
    "    scores = D[0].tolist()\n",
    "    ids = I[0].tolist()\n",
    "    rows = []\n",
    "    for rank, (score, idx_) in enumerate(zip(scores, ids), 1):\n",
    "        row = docs.iloc[idx_]\n",
    "        rows.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(score),\n",
    "            \"doc_id\": int(row.doc_id),\n",
    "            \"title\": row.title,\n",
    "            \"category\": row.category,\n",
    "            \"path\": row.path,\n",
    "            \"preview\": preview(row.text, 280)\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Quick smoke test\n",
    "search(\"How do I build a FAISS index for semantic search?\", k=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97137fcc",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Demonstration — Five diverse queries\n",
    "\n",
    "Run the cell below to see the ranked outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd404ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_queries = [\n",
    "    \"How do I vectorize text for semantic search?\",\n",
    "    \"Beginner tips for visiting Kyoto in spring\",\n",
    "    \"Common early symptoms of type 2 diabetes\",\n",
    "    \"ETF vs mutual fund: what are the key differences?\",\n",
    "    \"Troubleshooting slow SQL queries and index usage\"\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for q in example_queries:\n",
    "    print(\"\\n=== Query:\", q)\n",
    "    df = search(q, k=5)\n",
    "    display(df)\n",
    "    results[q] = df\n",
    "\n",
    "# Optionally: save a CSV of all demo results\n",
    "pd.concat([df.assign(query=q) for q, df in results.items()]).to_csv(\"demo_results.csv\", index=False)\n",
    "print(\"Saved demo results to demo_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883485b3",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Interactive search (optional)\n",
    "\n",
    "Run this cell and type queries. Press Enter on an empty line to stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    while True:\n",
    "        q = input(\"\\nEnter a query (blank to stop): \").strip()\n",
    "        if not q:\n",
    "            break\n",
    "        df = search(q, k=5)\n",
    "        display(df)\n",
    "except EOFError:\n",
    "    # Some environments don't support interactive input\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f603e11",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix — Save / Load\n",
    "\n",
    "This shows how to reload the saved artifacts later without recomputing everything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e590be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_cached() -> Tuple[pd.DataFrame, np.ndarray, object]:\n",
    "    docs_df = pd.read_parquet(META_CACHE)\n",
    "    embs = np.load(EMBED_CACHE)\n",
    "    if _faiss_available and INDEX_PATH.exists():\n",
    "        idx = faiss.read_index(str(INDEX_PATH))\n",
    "    else:\n",
    "        idx = NumpyFlatIndex(embs)\n",
    "    return docs_df, embs, idx\n",
    "\n",
    "# Example usage:\n",
    "# docs2, embs2, idx2 = load_cached()\n",
    "# display(docs2.head(2))\n",
    "# print(embs2.shape, type(idx2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3d3e5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "- **Cosine vs inner product**: If vectors are **L2-normalized**, maximizing inner product is equivalent to maximizing cosine similarity.\n",
    "- **Chunking**: Long documents are split into overlapping chunks and averaged so each doc yields a single vector.\n",
    "- **Quality**: For best relevance, use `text-embedding-3-small` or `text-embedding-3-large`. The fallback hashing embedding is for offline demos only.\n",
    "- **Scaling up**: For millions of docs, consider FAISS IVF/IVF+PQ or HNSW indexes and persist them to disk.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
